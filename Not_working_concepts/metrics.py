# -*- coding: utf-8 -*-
"""Metrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OkdyuBmF29wBN9saZTyhPJGL5Yt7n-FR
"""
import numpy as np
import random
from transformers import ResNetModel
from torch import nn
from torch.utils.data import Dataset,DataLoader
from PIL import Image
from torchvision.transforms import v2
import torch
import pandas as pd
import evaluate
from torch.utils.data import DataLoader
import zipfile
from tqdm import tqdm
import torch
import torch.optim as optim
from torchvision import transforms
from torchvision.models import resnet18
import random


DEVICE = 'cuda'

base_path = '/fhome/gia03/'
zip_file_path = f'{base_path}Dataset.zip'
extracted_folder_path = f'{base_path}extracted_data/'

# Extraer el contenido del archivo zip
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_folder_path)

# Ahora, puedes cargar tus datos desde la carpeta extra√≠da
img_path = f'{extracted_folder_path}Images/'
cap_path = f'{extracted_folder_path}captions.txt'

data = pd.read_csv(cap_path)
partitions = np.load("/fhome/gia03/flickr8k_partitions.npy", allow_pickle=True).item()

chars = ['<SOS>', '<EOS>', '<PAD>', ' ', '!', '"', '#', '&', "'", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

NUM_CHAR = len(chars)
idx2char = {k: v for k, v in enumerate(chars)}
char2idx = {v: k for k, v in enumerate(chars)}

TEXT_MAX_LEN = 201


'''metrics'''
bleu = evaluate.load('bleu')
meteor = evaluate.load('meteor')
rouge = evaluate.load('rouge')


def text_to_indices(text, char2idx):
    return [char2idx[char] for char in text]


class Data(Dataset):
    def __init__(self, data, partition):
        self.data = data
        self.partition = partition
        self.num_captions = 5
        self.max_len = TEXT_MAX_LEN
        self.img_proc = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((224, 224)),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def __len__(self):
        return len(self.partition)

    def __getitem__(self, idx):
        real_idx = self.num_captions * self.partition[idx]
        item = self.data.iloc[real_idx: real_idx + self.num_captions]
        ## image processing
        img_name = item.image.reset_index(drop=True)[0]
        img = Image.open(f'{img_path}{img_name}').convert('RGB')
        img = self.img_proc(img)

        ## caption processing
        caption = item.caption.reset_index(drop=True)[random.choice(list(range(self.num_captions)))]
        cap_list = list(caption)
        final_list = [chars[0]]
        final_list.extend(cap_list)
        final_list.extend([chars[2]])
        gap = self.max_len - len(final_list)
        final_list.extend([chars[2]] * gap)
        cap_idx = text_to_indices(final_list, char2idx)

        return img, torch.tensor(cap_idx, dtype=torch.long)



import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.resnet = ResNetModel.from_pretrained('microsoft/resnet-18').to(DEVICE)
        self.gru = nn.GRU(512, 512, num_layers=1)
        self.proj = nn.Linear(512, NUM_CHAR)
        self.embed = nn.Embedding(NUM_CHAR, 512)

    def forward(self, img):
        batch_size = img.shape[0]
        feat = self.resnet(img)
        feat = feat.pooler_output.squeeze(-1).squeeze(-1).unsqueeze(0) # 1, batch, 512
        start = torch.tensor(char2idx['<SOS>']).to(DEVICE)
        start_embed = self.embed(start) # 512
        start_embeds = start_embed.repeat(batch_size, 1).unsqueeze(0) # 1, batch, 512
        inp = start_embeds
        hidden = feat
        for t in range(TEXT_MAX_LEN-1): # rm <SOS>
            out, hidden = self.gru(inp, hidden)
            inp = torch.cat((inp, out[-1:]), dim=0) # N, batch, 512

        res = inp.permute(1, 0, 2) # batch, seq, 512
        res = self.proj(res) # batch, seq, 80
        res = res.permute(0, 2, 1) # batch, 80, seq
        return res

    def calculate_regularization_loss(self):
        reg_loss = 0.0
        for param in self.parameters():
            reg_loss += self.l2_regularization(param)
        return reg_loss * self.weight_decay

def id2str(seq):
    return ("".join([idx2char[item] for item in seq]))


def train_one_epoch(model, optimizer, criterion, dataloader):
    model.train()
    total_loss = 0

    for img_batch, caption_batch in tqdm(dataloader):
        img_batch, caption_batch = img_batch.to(DEVICE), caption_batch.to(DEVICE)
        optimizer.zero_grad()
        pred = model(img_batch)

        loss = criterion(pred.reshape(-1, NUM_CHAR), caption_batch.reshape(-1).contiguous())
        loss.backward()
        optimizer.step()
        total_loss += loss.item()


        pred = pred.argmax(dim=1).tolist()
        pred = [id2str(seq) for seq in pred]

        #import pdb;pdb.set_trace()
        # DEBUGGEAR

    avg_loss = total_loss / len(dataloader)
    return avg_loss


def eval_epoch(model, criterion, dataloader,optimizer):
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for img_batch, caption_batch in dataloader:
            img_batch, caption_batch = img_batch.to(DEVICE), caption_batch.to(DEVICE)

            pred = model(img_batch)

            #pred = pred.permute(0, 2, 1).contiguous().view(-1, NUM_CHAR)

            loss = criterion(pred.reshape(-1, NUM_CHAR), caption_batch.reshape(-1).contiguous())
            #loss.backward()
            total_loss += loss.item()


            pred = pred.argmax(dim=1).tolist()
            pred = [id2str(seq) for seq in pred]


    avg_loss = total_loss / len(dataloader)
    return avg_loss



def train(EPOCHS, model, optimizer, criterion, train_dataloader, valid_dataloader):
    for epoch in range(EPOCHS):
        train_loss = train_one_epoch(model, optimizer, criterion, train_dataloader)
        print(f'train loss: {train_loss:.2f}, epoch: {epoch}')


        valid_loss = eval_epoch(model, criterion, valid_dataloader,optimizer)
        print(f'valid loss: {valid_loss:.2f}')

if __name__ == "__main__":
    model = Model().to(DEVICE)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-3)
    criterion = nn.CrossEntropyLoss()
    
    train_dataloader = DataLoader(Data(data, partitions['train']), batch_size=16, shuffle=True)
    valid_dataloader = DataLoader(Data(data, partitions['valid']), batch_size=16, shuffle=True)
    
    train(EPOCHS=35, model=model, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader)
    
    torch.save(model.state_dict(),'/fhome/gia03/Gloria/model_epoch35.pth')
  
    pass

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f28f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/env_img_capt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from transformers import ResNetModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b723e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "\n",
    "base_path = '/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/'\n",
    "img_path = f'{base_path}Images/'\n",
    "cap_path = f'{base_path}captions.txt'\n",
    "\n",
    "data = pd.read_csv(cap_path)\n",
    "partitions = np.load('flickr8k_partitions.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b09b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['<SOS>', '<EOS>', '<PAD>', ' ', '!', '\"', '#', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "NUM_CHAR = len(chars)\n",
    "idx2char = {k: v for k, v in enumerate(chars)}\n",
    "char2idx = {v: k for k, v in enumerate(chars)}\n",
    "\n",
    "TEXT_MAX_LEN = 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff6590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, data, partition):\n",
    "        self.data = data\n",
    "        self.partition = partition\n",
    "        self.num_captions = 5\n",
    "        self.max_len = TEXT_MAX_LEN\n",
    "        self.img_proc = torch.nn.Sequential(\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize((224, 224), antialias=True),\n",
    "            v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.partition)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.num_captions*self.partition[idx]\n",
    "        item = self.data.iloc[real_idx: real_idx+self.num_captions]\n",
    "        ## image processing\n",
    "        img_name = item.image.reset_index(drop=True)[0]\n",
    "        img = Image.open(f'{img_path}{img_name}').convert('RGB')\n",
    "        img = self.img_proc(img)\n",
    "    \n",
    "        ## caption processing\n",
    "        caption = item.caption.reset_index(drop=True)[random.choice(list(range(self.num_captions)))]\n",
    "        cap_list = list(caption)\n",
    "        final_list = [chars[0]]\n",
    "        final_list.extend(cap_list)\n",
    "        final_list.extend([chars[1]])\n",
    "        gap = self.max_len - len(final_list)\n",
    "        final_list.extend([chars[2]]*gap)\n",
    "        cap_idx = [char2idx[i] for i in final_list]\n",
    "        return img, cap_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b652abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = ResNetModel.from_pretrained('microsoft/resnet-18').to(DEVICE)\n",
    "        self.gru = nn.GRU(512, 512, num_layers=1)\n",
    "        self.proj = nn.Linear(512, NUM_CHAR)\n",
    "        self.embed = nn.Embedding(NUM_CHAR, 512)\n",
    "\n",
    "    def forward(self, img):\n",
    "        batch_size = img.shape[0]\n",
    "        feat = self.resnet(img)\n",
    "        feat = feat.pooler_output.squeeze(-1).squeeze(-1).unsqueeze(0) # 1, batch, 512\n",
    "        start = torch.tensor(char2idx['<SOS>']).to(DEVICE)\n",
    "        start_embed = self.embed(start) # 512\n",
    "        start_embeds = start_embed.repeat(batch_size, 1).unsqueeze(0) # 1, batch, 512\n",
    "        inp = start_embeds\n",
    "        hidden = feat\n",
    "        for t in range(TEXT_MAX_LEN-1): # rm <SOS>\n",
    "            out, hidden = self.gru(inp, hidden)\n",
    "            inp = torch.cat((inp, out[-1:]), dim=0) # N, batch, 512\n",
    "    \n",
    "        res = inp.permute(1, 0, 2) # batch, seq, 512\n",
    "        res = self.proj(res) # batch, seq, 80\n",
    "        res = res.permute(0, 2, 1) # batch, 80, seq\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82acf476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([ 0, 47, 61, 58,  3, 57, 68, 60,  3, 62, 72,  3, 71, 74, 67, 67, 62, 67,\n",
      "        60,  3, 76, 62, 73, 61,  3, 59, 68, 68, 57,  3, 62, 67,  3, 61, 62, 72,\n",
      "         3, 66, 68, 74, 73, 61,  3, 13,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3962, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''A simple example to calculate loss of a single batch (size 2)'''\n",
    "dataset = Data(data, partitions['train'])\n",
    "img1, caption1 = next(iter(dataset))\n",
    "\n",
    "img2, caption2 = next(iter(dataset))\n",
    "caption1 = torch.tensor(caption1)\n",
    "caption2 = torch.tensor(caption2)\n",
    "print(type(caption2))\n",
    "print(caption2)\n",
    "\n",
    "img = torch.cat((img1.unsqueeze(0), img2.unsqueeze(0)))\n",
    "caption = torch.cat((caption1.unsqueeze(0), caption2.unsqueeze(0)))\n",
    "img, caption = img.to(DEVICE), caption.to(DEVICE)\n",
    "model = Model().to(DEVICE)\n",
    "pred = model(img)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "loss = crit(pred, caption)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3606f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sisard/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sisard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sisard/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.5946035575013605,\n",
       "  'precisions': [0.875, 0.7142857142857143, 0.5, 0.4],\n",
       "  'brevity_penalty': 1.0,\n",
       "  'length_ratio': 1.0,\n",
       "  'translation_length': 8,\n",
       "  'reference_length': 8},\n",
       " {'rouge1': 0.8571428571428571,\n",
       "  'rouge2': 0.6666666666666666,\n",
       "  'rougeL': 0.8571428571428571,\n",
       "  'rougeLsum': 0.8571428571428571},\n",
       " {'meteor': 0.864795918367347})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''metrics'''\n",
    "bleu = evaluate.load('bleu')\n",
    "meteor = evaluate.load('meteor')\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "reference = [['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .']]\n",
    "prediction = ['A girl goes into a wooden building .']\n",
    "\n",
    "res_b = bleu.compute(predictions=prediction, references=reference)\n",
    "res_r = rouge.compute(predictions=prediction, references=reference)\n",
    "res_m = meteor.compute(predictions=prediction, references=reference)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7595f1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.4723665527410147,\n",
       "  'precisions': [1.0, 1.0, 1.0, 1.0],\n",
       "  'brevity_penalty': 0.4723665527410147,\n",
       "  'length_ratio': 0.5714285714285714,\n",
       "  'translation_length': 4,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.7272727272727273,\n",
       "  'rouge2': 0.6666666666666666,\n",
       "  'rougeL': 0.7272727272727273,\n",
       "  'rougeLsum': 0.7272727272727273},\n",
       " {'meteor': 0.5923507462686567})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child is running']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe24da77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.0,\n",
       "  'precisions': [1.0, 1.0, 1.0, 0.0],\n",
       "  'brevity_penalty': 0.2635971381157267,\n",
       "  'length_ratio': 0.42857142857142855,\n",
       "  'translation_length': 3,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.6, 'rouge2': 0.5, 'rougeL': 0.6, 'rougeLsum': 0.6},\n",
       " {'meteor': 0.44612794612794615})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child is']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "res_b, res_r, res_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca1afd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bleu': 0.0,\n",
       "  'precisions': [1.0, 0.5, 0.0, 0.0],\n",
       "  'brevity_penalty': 0.2635971381157267,\n",
       "  'length_ratio': 0.42857142857142855,\n",
       "  'translation_length': 3,\n",
       "  'reference_length': 7},\n",
       " {'rouge1': 0.6, 'rouge2': 0.25, 'rougeL': 0.6, 'rougeLsum': 0.6},\n",
       " {'meteor': 0.3872053872053872},\n",
       " {'meteor': 0.45454545454545453})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child campus']\n",
    "\n",
    "res_b = bleu.compute(predictions=pred1, references=ref)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "res_m_sin = meteor.compute(predictions=pred1, references=ref, gamma=0) # no penalty by setting gamma to 0\n",
    "\n",
    "res_b, res_r, res_m, res_m_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bac05",
   "metadata": {},
   "source": [
    "Final metric we use for challenge 3: BLEU1, BLEU2, ROUGE-L, METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5433823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BLEU-1:26.4%, BLEU2:18.6%, ROUGE-L:60.0%, METEOR:38.7%'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [['A child is running in the campus']]\n",
    "pred1 = ['A child campus']\n",
    "\n",
    "bleu1 = bleu.compute(predictions=pred1, references=ref, max_order=1)\n",
    "bleu2 = bleu.compute(predictions=pred1, references=ref, max_order=2)\n",
    "res_r = rouge.compute(predictions=pred1, references=ref)\n",
    "res_m = meteor.compute(predictions=pred1, references=ref)\n",
    "\n",
    "f\"BLEU-1:{bleu1['bleu']*100:.1f}%, BLEU2:{bleu2['bleu']*100:.1f}%, ROUGE-L:{res_r['rougeL']*100:.1f}%, METEOR:{res_m['meteor']*100:.1f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf5d9d",
   "metadata": {},
   "source": [
    "Now it is your turn! Try to finish the code below to run the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9835fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16615/2368983450.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  caption = torch.tensor(caption)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··························\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 80, 201])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [1, 201], got [1, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m             res \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpred, references\u001b[39m=\u001b[39mcap)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, res\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m train(EPOCHS\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m metric \u001b[39m=\u001b[39m rouge\u001b[39m#funciona?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss, res \u001b[39m=\u001b[39m train_one_epoch(model, optimizer, crit, metric, dataloader_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, metric: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss_v, res_v \u001b[39m=\u001b[39m eval_epoch(model, crit, metric, dataloader_valid)\n",
      "\u001b[1;32m/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(caption\u001b[39m.\u001b[39msize())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(pred\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m loss \u001b[39m=\u001b[39m crit(pred, caption)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/env_img_capt/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/env_img_capt/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/env_img_capt/lib/python3.9/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/env_img_capt/lib/python3.9/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [1, 201], got [1, 64]"
     ]
    }
   ],
   "source": [
    "def train(EPOCHS):\n",
    "    data_train = Data(data, partitions['train'])\n",
    "    data_valid = Data(data, partitions['valid'])\n",
    "    data_test = Data(data, partitions['test'])\n",
    "    dataloader_train = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "    dataloader_test = DataLoader(data_valid, batch_size=64, shuffle=True)\n",
    "    dataloader_valid = DataLoader(data_test, batch_size=64, shuffle=True)\n",
    "    \n",
    "    model = Model().to(DEVICE)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    metric = rouge#funciona?\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss, res = train_one_epoch(model, optimizer, crit, metric, dataloader_train)\n",
    "        print(f'train loss: {loss:.2f}, metric: {res:.2f}, epoch: {epoch}')\n",
    "        loss_v, res_v = eval_epoch(model, crit, metric, dataloader_valid)\n",
    "        print(f'valid loss: {loss:.2f}, metric: {res:.2f}')\n",
    "    loss_t, res_t = eval_epoch(model, crit, metric, dataloader_test)\n",
    "    print(f'test loss: {loss:.2f}, metric: {res:.2f}')\n",
    "    \n",
    "def train_one_epoch(model, optimizer, crit, metric, dataloader):\n",
    "    for image, cap in dataloader:\n",
    "        for img, caption in zip(image,cap):\n",
    "            print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')\n",
    "            print(type(caption))\n",
    "            print(caption)\n",
    "            caption = torch.tensor(caption)\n",
    "            img = img.unsqueeze(0)\n",
    "            #img = torch.cat(img.unsqueeze(0))\n",
    "            caption = caption.unsqueeze(0)\n",
    "            #caption = torch.cat(caption.unsqueeze(0))\n",
    "            img, cap = img.to(DEVICE), caption.to(DEVICE)\n",
    "            pred = model(img)\n",
    "            print('··························')\n",
    "            print(caption.size())\n",
    "            print(pred.size())\n",
    "            loss = crit(pred, caption)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            res = metric.compute(predictions=pred, references=caption)\n",
    "\n",
    "    return loss, res\n",
    "\n",
    "def eval_epoch(model, crit, metric, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, cap in dataloader:\n",
    "            pred = model(img)\n",
    "            loss = crit(pred,cap)\n",
    "            res = metric.compute(predictions=pred, references=cap)\n",
    "\n",
    "\n",
    "    return loss, res\n",
    "train(EPOCHS=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(EPOCHS\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m metric \u001b[39m=\u001b[39m rouge\u001b[39m#funciona?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss, res \u001b[39m=\u001b[39m train_one_epoch(model, optimizer, crit, metric, dataloader_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, metric: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss_v, res_v \u001b[39m=\u001b[39m eval_epoch(model, crit, metric, dataloader_valid)\n",
      "\u001b[1;32m/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_one_epoch\u001b[39m(model, optimizer, crit, metric, dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mfor\u001b[39;00m img, cap \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         cap \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(cap)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39m#img = torch.cat((img1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sisard/Documents/universitat/year_3/semester_1/Computer_Vision/Image_Captioning/Metrics.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         cap \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((cap\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), caption2\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "train(EPOCHS=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
